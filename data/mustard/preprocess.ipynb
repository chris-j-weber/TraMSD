{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#import os\n",
    "import cv2\n",
    "import random\n",
    "import torch\n",
    "#import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mustard++_text.csv')\n",
    "# df.drop(['END_TIME', 'SPEAKER', 'SHOW', 'Sarcasm_Type', 'Valence', 'Arousal'],axis=1,inplace=True)\n",
    "#df.drop(['END_TIME', 'Sarcasm_Type', 'Valence', 'Arousal'],axis=1,inplace=True)\n",
    "#df.dropna(subset=['Sarcasm'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = []\n",
    "\n",
    "list_of_text = []\n",
    "for index, row in df.iterrows():\n",
    "    if row['Sarcasm'] in [0.0, 1.0]:\n",
    "      text = row['SENTENCE']\n",
    "      text = re.sub(\"[\\n]\", \" \", text)\n",
    "      list_of_text.append(text)\n",
    "\n",
    "      tmp = {'key': row['SCENE'], \n",
    "             'image': row['KEY'], \n",
    "             'text': list_of_text,\n",
    "             'label': row['Sarcasm']}\n",
    "\n",
    "      data_dict.append(tmp)\n",
    "      list_of_text = []\n",
    "    else:\n",
    "      text = row['SENTENCE']\n",
    "      text = re.sub(\"[\\n]\", \" \", text)\n",
    "      list_of_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_dict:\n",
    "  i['label'] = int(i['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_frame(frame):\n",
    "    return frame is not None and frame.size > 0\n",
    "\n",
    "failed_data_points = []\n",
    "\n",
    "videos = []\n",
    "text = []\n",
    "labels = []\n",
    "ids = []\n",
    "\n",
    "down_width = 384\n",
    "down_height = 224\n",
    "down_points = (down_width, down_height)\n",
    "\n",
    "num_frames = 16\n",
    "for data in data_dict[:]:\n",
    "    video_id = data['image']\n",
    "    video_path = 'videos/final_utterance_videos/'+video_id+'.mp4'\n",
    "    cam = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cam.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # skip data point which are shorter than num_frames\n",
    "    if total_frames < num_frames:\n",
    "        failed_data_points.append(video_path)\n",
    "        continue\n",
    "\n",
    "    random_frame_idxs = random.sample(range(total_frames), num_frames)\n",
    "\n",
    "    frames = []\n",
    "    for idx, frame_idx in enumerate(sorted(random_frame_idxs)):\n",
    "        valid_frame = False\n",
    "        attempts = 0 \n",
    "        \n",
    "        while not valid_frame and attempts < 3:\n",
    "            cam.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            ret, frame = cam.read()\n",
    "\n",
    "            if ret and is_valid_frame(frame):\n",
    "                resized_frame = cv2.resize(frame, down_points, interpolation=cv2.INTER_LINEAR)\n",
    "                frames.append(resized_frame)\n",
    "                valid_frame = True\n",
    "            else:\n",
    "                attempts += 1\n",
    "                if frame_idx < total_frames - 1:\n",
    "                    frame_idx += 1\n",
    "                else:\n",
    "                    frame_idx -= 1\n",
    "\n",
    "    # if any frames are corrupted, skip data point\n",
    "    if len(frames) < num_frames:\n",
    "        failed_data_points.append(video_path)\n",
    "        continue\n",
    "\n",
    "    # print(f'video: {video_id}, frames {len(frames)}')\n",
    "\n",
    "    video = np.array(frames)\n",
    "    tensor_video = torch.from_numpy(video)\n",
    "    videos.append(tensor_video)\n",
    "\n",
    "    text.append(data['text'])\n",
    "    labels.append(data['label'])\n",
    "    ids.append(data['key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample (show dependent - speaker independet) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for index, row in df.iterrows():\n",
    "    if row['SHOW'] in ['BBT', 'SV']:\n",
    "        train_data.append(row['SCENE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_video = []\n",
    "train_text = []\n",
    "train_label = []\n",
    "train_id = []\n",
    "rest_videos = []\n",
    "rest_text = []\n",
    "rest_labels = []\n",
    "rest_ids = []\n",
    "\n",
    "for index, id in enumerate(ids):\n",
    "    if id in train_data:\n",
    "        train_video.append(videos[index])\n",
    "        train_text.append(text[index])\n",
    "        train_label.append(labels[index])\n",
    "        train_id.append(ids[index])\n",
    "    else:\n",
    "        rest_videos.append(videos[index])\n",
    "        rest_text.append(text[index])\n",
    "        rest_labels.append(labels[index])\n",
    "        rest_ids.append(ids[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text, test_text, val_video, test_video, val_label, test_label, val_id, test_id = train_test_split(rest_text, rest_videos, rest_labels, rest_ids, test_size=0.5, stratify=rest_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample balanced (train, val, test) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, temp_text, train_video, temp_video, train_label, temp_label, train_id, temp_id = train_test_split(text, videos, labels, ids, test_size=0.2, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text, test_text, val_video, test_video, val_label, test_label, val_id, test_id = train_test_split(temp_text, temp_video, temp_label, temp_id, test_size=0.5, stratify=temp_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample random (train, val, test) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.arange(len(labels))\n",
    "np.random.shuffle(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val_test = train_test_split(index, test_size=0.2)\n",
    "val, test = train_test_split(val_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [text[i] for i in test]\n",
    "test_video = [videos[i] for i in test]\n",
    "test_label = [labels[i] for i in test]\n",
    "test_id = [ids[i] for i in test]\n",
    "\n",
    "train_text = [text[i] for i in train]\n",
    "train_video = [videos[i] for i in train]\n",
    "train_label = [labels[i] for i in train]\n",
    "train_id = [ids[i] for i in train]\n",
    "\n",
    "val_text = [text[i] for i in val]\n",
    "val_video = [videos[i] for i in val]\n",
    "val_label = [labels[i] for i in val]\n",
    "val_id = [ids[i] for i in val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_video, f\"preprocessed/video_train.pt\")\n",
    "torch.save(train_text, f\"preprocessed/text_train.pt\")\n",
    "torch.save(train_label, f\"preprocessed/labels_train.pt\")\n",
    "torch.save(train_id, f\"preprocessed/ids_train.pt\")\n",
    "\n",
    "torch.save(val_video, f\"preprocessed/video_val.pt\")\n",
    "torch.save(val_text, f\"preprocessed/text_val.pt\")\n",
    "torch.save(val_label, f\"preprocessed/labels_val.pt\")\n",
    "torch.save(val_id, f\"preprocessed/ids_val.pt\")\n",
    "\n",
    "torch.save(test_video, f\"preprocessed/video_test.pt\")\n",
    "torch.save(test_text, f\"preprocessed/text_test.pt\")\n",
    "torch.save(test_label, f\"preprocessed/labels_test.pt\")\n",
    "torch.save(test_id, f\"preprocessed/ids_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_video, f\"preprocessed/video_train_2.pt\")\n",
    "torch.save(train_text, f\"preprocessed/text_train_2.pt\")\n",
    "torch.save(train_label, f\"preprocessed/labels_train_2.pt\")\n",
    "torch.save(train_id, f\"preprocessed/ids_train_2.pt\")\n",
    "\n",
    "torch.save(val_video, f\"preprocessed/video_val_2.pt\")\n",
    "torch.save(val_text, f\"preprocessed/text_val_2.pt\")\n",
    "torch.save(val_label, f\"preprocessed/labels_val_2.pt\")\n",
    "torch.save(val_id, f\"preprocessed/ids_val_2.pt\")\n",
    "\n",
    "torch.save(test_video, f\"preprocessed/video_test_2.pt\")\n",
    "torch.save(test_text, f\"preprocessed/text_test_2.pt\")\n",
    "torch.save(test_label, f\"preprocessed/labels_test_2.pt\")\n",
    "torch.save(test_id, f\"preprocessed/ids_test_2.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
